Now you should run one of the following depending on your shell
source /share/apps/python/anaconda3.2019.3/etc/profile.d/conda.sh
source /share/apps/python/anaconda3.2019.3/etc/profile.d/conda.csh
Running model_type=bprmf on all datasetsFri Jan  8 14:44:41 PST 2021
Running model_type=bprmf on dataset=amazon-book
Num cores: 20
[n_users, n_items]=[70679, 24915]
[n_train, n_test]=[652514, 193920]
[n_entities, n_relations, n_triples]=[113487, 39, 2557746]
[batch_size, batch_size_kg]=[1024, 4015]
load the pretrained bprmf model parameters.
using pretrained initialization
#params: 6118016
without pretraining.
Epoch 0 [22.6s]: train==[70.31083=21.57110 + 0.00000 + 48.73976]
Epoch 9 [21.1s + 55.8s]: train==[69.98734=21.42807 + 0.00000 + 48.55923], recall=[0.13085, 0.29843], precision=[0.01387, 0.00694], hit=[0.23043, 0.46764], ndcg=[0.08926, 0.14373]
save the weights in path:  weights/amazon-book/mf/l0.0001_r1e-05-1e-05
Epoch 19 [22.1s + 54.0s]: train==[69.90942=21.43464 + 0.00000 + 48.47477], recall=[0.13145, 0.29985], precision=[0.01387, 0.00697], hit=[0.23067, 0.46968], ndcg=[0.08914, 0.14383]
save the weights in path:  weights/amazon-book/mf/l0.0001_r1e-05-1e-05
Epoch 29 [22.2s + 54.5s]: train==[69.74048=21.32316 + 0.00000 + 48.41730], recall=[0.13115, 0.30016], precision=[0.01387, 0.00698], hit=[0.23091, 0.47056], ndcg=[0.08879, 0.14369]
Epoch 39 [22.7s + 54.3s]: train==[69.63639=21.26315 + 0.00000 + 48.37324], recall=[0.13210, 0.30106], precision=[0.01396, 0.00699], hit=[0.23244, 0.47162], ndcg=[0.08946, 0.14425]
save the weights in path:  weights/amazon-book/mf/l0.0001_r1e-05-1e-05
Epoch 49 [23.2s + 55.1s]: train==[69.55673=21.24269 + 0.00000 + 48.31402], recall=[0.13187, 0.30161], precision=[0.01392, 0.00702], hit=[0.23190, 0.47223], ndcg=[0.08921, 0.14437]
Epoch 50 [21.5s]: train==[69.54583=21.21596 + 0.00000 + 48.32987]
Epoch 59 [22.5s + 54.1s]: train==[69.41521=21.11626 + 0.00000 + 48.29897], recall=[0.13152, 0.30200], precision=[0.01391, 0.00701], hit=[0.23180, 0.47383], ndcg=[0.08881, 0.14425]
Epoch 69 [22.1s + 54.0s]: train==[69.38960=21.11270 + 0.00000 + 48.27693], recall=[0.13220, 0.30279], precision=[0.01395, 0.00704], hit=[0.23223, 0.47398], ndcg=[0.08925, 0.14470]
save the weights in path:  weights/amazon-book/mf/l0.0001_r1e-05-1e-05
Epoch 79 [22.2s + 55.6s]: train==[69.29324=21.04065 + 0.00000 + 48.25254], recall=[0.13205, 0.30331], precision=[0.01395, 0.00705], hit=[0.23231, 0.47472], ndcg=[0.08952, 0.14510]
Epoch 89 [21.1s + 54.4s]: train==[69.27921=21.04987 + 0.00000 + 48.22939], recall=[0.13271, 0.30457], precision=[0.01400, 0.00708], hit=[0.23347, 0.47643], ndcg=[0.08956, 0.14530]
save the weights in path:  weights/amazon-book/mf/l0.0001_r1e-05-1e-05
Epoch 99 [22.7s + 55.2s]: train==[69.21972=21.00677 + 0.00000 + 48.21294], recall=[0.13292, 0.30365], precision=[0.01401, 0.00705], hit=[0.23332, 0.47512], ndcg=[0.08955, 0.14502]
save the weights in path:  weights/amazon-book/mf/l0.0001_r1e-05-1e-05
Epoch 100 [22.1s]: train==[69.17226=20.96645 + 0.00000 + 48.20582]
Epoch 109 [21.9s + 54.5s]: train==[69.29198=21.09778 + 0.00000 + 48.19424], recall=[0.13206, 0.30362], precision=[0.01391, 0.00705], hit=[0.23190, 0.47526], ndcg=[0.08851, 0.14427]
Epoch 119 [22.3s + 54.5s]: train==[69.12471=20.94987 + 0.00000 + 48.17484], recall=[0.13205, 0.30399], precision=[0.01394, 0.00706], hit=[0.23228, 0.47568], ndcg=[0.08924, 0.14507]
Epoch 129 [23.0s + 56.1s]: train==[69.05181=20.89864 + 0.00000 + 48.15318], recall=[0.13149, 0.30466], precision=[0.01386, 0.00707], hit=[0.23136, 0.47690], ndcg=[0.08868, 0.14496]
Epoch 139 [22.6s + 54.1s]: train==[69.10124=20.93993 + 0.00000 + 48.16133], recall=[0.13304, 0.30518], precision=[0.01403, 0.00709], hit=[0.23436, 0.47757], ndcg=[0.08967, 0.14562]
save the weights in path:  weights/amazon-book/mf/l0.0001_r1e-05-1e-05
Epoch 149 [22.4s + 53.6s]: train==[69.02328=20.87145 + 0.00000 + 48.15183], recall=[0.13192, 0.30483], precision=[0.01389, 0.00707], hit=[0.23211, 0.47716], ndcg=[0.08878, 0.14502]
Epoch 150 [23.2s]: train==[68.98094=20.84021 + 0.00000 + 48.14076]
Epoch 159 [22.4s + 53.7s]: train==[69.15582=21.04240 + 0.00000 + 48.11344], recall=[0.13279, 0.30540], precision=[0.01397, 0.00708], hit=[0.23305, 0.47791], ndcg=[0.08925, 0.14537]
Epoch 169 [21.8s + 54.4s]: train==[68.92863=20.80644 + 0.00000 + 48.12221], recall=[0.13272, 0.30570], precision=[0.01397, 0.00709], hit=[0.23308, 0.47815], ndcg=[0.08908, 0.14532]
Epoch 179 [23.8s + 54.8s]: train==[68.86005=20.72836 + 0.00000 + 48.13167], recall=[0.13373, 0.30629], precision=[0.01410, 0.00711], hit=[0.23555, 0.47881], ndcg=[0.09019, 0.14621]
save the weights in path:  weights/amazon-book/mf/l0.0001_r1e-05-1e-05
Epoch 189 [23.4s + 55.1s]: train==[68.85725=20.74329 + 0.00000 + 48.11397], recall=[0.13190, 0.30534], precision=[0.01389, 0.00708], hit=[0.23232, 0.47743], ndcg=[0.08902, 0.14526]
Epoch 199 [22.9s + 55.9s]: train==[68.79752=20.68122 + 0.00000 + 48.11633], recall=[0.13381, 0.30655], precision=[0.01409, 0.00711], hit=[0.23499, 0.47881], ndcg=[0.08980, 0.14584]
save the weights in path:  weights/amazon-book/mf/l0.0001_r1e-05-1e-05
Epoch 200 [22.4s]: train==[69.02103=20.90775 + 0.00000 + 48.11330]
Epoch 209 [23.3s + 55.0s]: train==[68.92388=20.82352 + 0.00000 + 48.10034], recall=[0.13293, 0.30679], precision=[0.01401, 0.00711], hit=[0.23383, 0.47904], ndcg=[0.08963, 0.14592]
Epoch 219 [23.0s + 54.8s]: train==[68.85856=20.77156 + 0.00000 + 48.08699], recall=[0.13324, 0.30566], precision=[0.01403, 0.00709], hit=[0.23405, 0.47733], ndcg=[0.08913, 0.14506]
Epoch 229 [23.5s + 54.7s]: train==[68.79547=20.70462 + 0.00000 + 48.09085], recall=[0.13340, 0.30634], precision=[0.01405, 0.00710], hit=[0.23476, 0.47847], ndcg=[0.08963, 0.14574]
Epoch 239 [22.5s + 53.6s]: train==[68.80677=20.74028 + 0.00000 + 48.06650], recall=[0.13344, 0.30637], precision=[0.01406, 0.00710], hit=[0.23516, 0.47873], ndcg=[0.08941, 0.14554]
Epoch 249 [23.0s + 53.6s]: train==[68.77738=20.72189 + 0.00000 + 48.05549], recall=[0.13294, 0.30665], precision=[0.01404, 0.00712], hit=[0.23480, 0.47947], ndcg=[0.08966, 0.14604]
Epoch 250 [22.5s]: train==[68.73167=20.67402 + 0.00000 + 48.05764]
Epoch 259 [21.9s + 54.6s]: train==[68.70174=20.64496 + 0.00000 + 48.05681], recall=[0.13382, 0.30802], precision=[0.01411, 0.00714], hit=[0.23594, 0.48136], ndcg=[0.08999, 0.14650]
save the weights in path:  weights/amazon-book/mf/l0.0001_r1e-05-1e-05
Epoch 269 [22.2s + 53.8s]: train==[68.66994=20.60807 + 0.00000 + 48.06187], recall=[0.13281, 0.30721], precision=[0.01398, 0.00712], hit=[0.23395, 0.48063], ndcg=[0.08963, 0.14628]
Epoch 279 [22.5s + 54.4s]: train==[68.57523=20.54797 + 0.00000 + 48.02726], recall=[0.13431, 0.30787], precision=[0.01415, 0.00713], hit=[0.23628, 0.48073], ndcg=[0.09029, 0.14654]
save the weights in path:  weights/amazon-book/mf/l0.0001_r1e-05-1e-05
Epoch 289 [22.8s + 54.8s]: train==[68.54562=20.50882 + 0.00000 + 48.03680], recall=[0.13279, 0.30680], precision=[0.01398, 0.00711], hit=[0.23378, 0.47959], ndcg=[0.08940, 0.14583]
Epoch 299 [22.0s + 55.0s]: train==[68.57279=20.53053 + 0.00000 + 48.04220], recall=[0.13455, 0.30845], precision=[0.01418, 0.00714], hit=[0.23642, 0.48138], ndcg=[0.09009, 0.14631]
save the weights in path:  weights/amazon-book/mf/l0.0001_r1e-05-1e-05
Epoch 300 [22.0s]: train==[68.63290=20.60134 + 0.00000 + 48.03151]
Epoch 309 [22.2s + 54.8s]: train==[68.55679=20.52768 + 0.00000 + 48.02909], recall=[0.13346, 0.30900], precision=[0.01408, 0.00716], hit=[0.23506, 0.48272], ndcg=[0.08984, 0.14670]
Epoch 319 [22.2s + 55.1s]: train==[68.64387=20.61936 + 0.00000 + 48.02449], recall=[0.13350, 0.30918], precision=[0.01404, 0.00716], hit=[0.23500, 0.48258], ndcg=[0.08956, 0.14646]
Epoch 329 [22.6s + 55.1s]: train==[68.59500=20.58090 + 0.00000 + 48.01407], recall=[0.13355, 0.30845], precision=[0.01404, 0.00715], hit=[0.23462, 0.48148], ndcg=[0.08935, 0.14602]
Epoch 339 [23.0s + 54.9s]: train==[68.62975=20.60845 + 0.00000 + 48.02129], recall=[0.13540, 0.30877], precision=[0.01423, 0.00715], hit=[0.23808, 0.48179], ndcg=[0.09049, 0.14673]
save the weights in path:  weights/amazon-book/mf/l0.0001_r1e-05-1e-05
Epoch 349 [22.3s + 54.8s]: train==[68.58965=20.57531 + 0.00000 + 48.01436], recall=[0.13446, 0.30947], precision=[0.01413, 0.00717], hit=[0.23602, 0.48329], ndcg=[0.09023, 0.14694]
Epoch 350 [21.9s]: train==[68.46864=20.46389 + 0.00000 + 48.00475]
Epoch 359 [22.6s + 54.9s]: train==[68.50583=20.50709 + 0.00000 + 47.99878], recall=[0.13515, 0.30895], precision=[0.01419, 0.00715], hit=[0.23749, 0.48284], ndcg=[0.09049, 0.14689]
Epoch 369 [22.0s + 54.8s]: train==[68.49045=20.51225 + 0.00000 + 47.97823], recall=[0.13459, 0.30882], precision=[0.01416, 0.00715], hit=[0.23730, 0.48224], ndcg=[0.08997, 0.14642]
Epoch 379 [21.9s + 54.8s]: train==[68.59212=20.60290 + 0.00000 + 47.98925], recall=[0.13385, 0.31012], precision=[0.01407, 0.00718], hit=[0.23554, 0.48393], ndcg=[0.08925, 0.14633]
Epoch 389 [23.6s + 55.2s]: train==[68.33310=20.37471 + 0.00000 + 47.95842], recall=[0.13426, 0.30916], precision=[0.01412, 0.00716], hit=[0.23601, 0.48288], ndcg=[0.08955, 0.14626]
Epoch 399 [21.4s + 54.8s]: train==[68.47857=20.50758 + 0.00000 + 47.97099], recall=[0.13556, 0.30935], precision=[0.01426, 0.00716], hit=[0.23863, 0.48301], ndcg=[0.09048, 0.14672]
save the weights in path:  weights/amazon-book/mf/l0.0001_r1e-05-1e-05
Epoch 400 [23.2s]: train==[68.42661=20.44132 + 0.00000 + 47.98528]
Epoch 409 [22.4s + 55.1s]: train==[68.38704=20.42565 + 0.00000 + 47.96137], recall=[0.13489, 0.30932], precision=[0.01419, 0.00716], hit=[0.23738, 0.48335], ndcg=[0.09060, 0.14714]
Epoch 419 [22.2s + 55.0s]: train==[68.40768=20.43853 + 0.00000 + 47.96914], recall=[0.13450, 0.31015], precision=[0.01413, 0.00718], hit=[0.23642, 0.48408], ndcg=[0.08967, 0.14652]
Epoch 429 [22.8s + 55.3s]: train==[68.42690=20.46617 + 0.00000 + 47.96070], recall=[0.13446, 0.30974], precision=[0.01414, 0.00716], hit=[0.23619, 0.48352], ndcg=[0.08993, 0.14668]
Epoch 439 [22.5s + 55.5s]: train==[68.23702=20.28796 + 0.00000 + 47.94905], recall=[0.13486, 0.31078], precision=[0.01414, 0.00719], hit=[0.23662, 0.48496], ndcg=[0.09050, 0.14758]
Epoch 449 [18.2s + 51.5s]: train==[68.28548=20.34279 + 0.00000 + 47.94269], recall=[0.13587, 0.31008], precision=[0.01427, 0.00718], hit=[0.23866, 0.48387], ndcg=[0.09072, 0.14725]
save the weights in path:  weights/amazon-book/mf/l0.0001_r1e-05-1e-05
Epoch 450 [22.1s]: train==[68.30609=20.35799 + 0.00000 + 47.94808]
Epoch 459 [23.1s + 54.1s]: train==[68.28374=20.35419 + 0.00000 + 47.92955], recall=[0.13534, 0.30969], precision=[0.01421, 0.00716], hit=[0.23764, 0.48370], ndcg=[0.09067, 0.14731]
Epoch 469 [23.2s + 54.7s]: train==[68.51447=20.57404 + 0.00000 + 47.94041], recall=[0.13425, 0.31093], precision=[0.01412, 0.00719], hit=[0.23613, 0.48543], ndcg=[0.08993, 0.14724]
Epoch 479 [22.1s + 53.9s]: train==[68.35233=20.42811 + 0.00000 + 47.92424], recall=[0.13479, 0.31118], precision=[0.01419, 0.00719], hit=[0.23755, 0.48583], ndcg=[0.09025, 0.14738]
Epoch 489 [21.8s + 54.9s]: train==[68.29824=20.38201 + 0.00000 + 47.91620], recall=[0.13398, 0.31007], precision=[0.01404, 0.00718], hit=[0.23506, 0.48427], ndcg=[0.08933, 0.14647]
Epoch 499 [22.3s + 54.1s]: train==[68.34901=20.42791 + 0.00000 + 47.92111], recall=[0.13546, 0.31126], precision=[0.01423, 0.00720], hit=[0.23768, 0.48628], ndcg=[0.09001, 0.14712]
Epoch 500 [23.1s]: train==[68.23585=20.32043 + 0.00000 + 47.91542]
Epoch 509 [22.0s + 54.9s]: train==[68.25133=20.33884 + 0.00000 + 47.91250], recall=[0.13514, 0.31166], precision=[0.01418, 0.00720], hit=[0.23711, 0.48688], ndcg=[0.09007, 0.14727]
Epoch 519 [22.3s + 55.1s]: train==[68.21091=20.28984 + 0.00000 + 47.92106], recall=[0.13561, 0.31116], precision=[0.01425, 0.00721], hit=[0.23829, 0.48577], ndcg=[0.09039, 0.14740]
Epoch 529 [23.0s + 54.5s]: train==[68.23860=20.33332 + 0.00000 + 47.90529], recall=[0.13498, 0.31091], precision=[0.01417, 0.00719], hit=[0.23703, 0.48515], ndcg=[0.08973, 0.14681]
Epoch 539 [23.2s + 54.8s]: train==[68.30231=20.39221 + 0.00000 + 47.91010], recall=[0.13572, 0.31323], precision=[0.01427, 0.00724], hit=[0.23819, 0.48792], ndcg=[0.09028, 0.14768]
Epoch 549 [23.0s + 54.1s]: train==[68.08125=20.19998 + 0.00000 + 47.88128], recall=[0.13457, 0.31195], precision=[0.01411, 0.00722], hit=[0.23642, 0.48669], ndcg=[0.08969, 0.14721]
Early stopping is trigger at step: 10 log:0.13457307338891192
Best Iter=[44]@[15392.5]	recall=[0.13587	0.19921	0.24414	0.28072	0.31008], precision=[0.01427	0.01088	0.00912	0.00800	0.00718], hit=[0.23866	0.33635	0.40015	0.44768	0.48387], ndcg=[0.09072	0.11310	0.12754	0.13855	0.14725]
Fri Jan  8 19:01:44 PST 2021
Running model_type=bprmf on dataset=last-fm
Num cores: 20
[n_users, n_items]=[23566, 48123]
[n_train, n_test]=[1289003, 423635]
[n_entities, n_relations, n_triples]=[106389, 9, 464567]
[batch_size, batch_size_kg]=[1024, 369]
load the pretrained bprmf model parameters.
using pretrained initialization
#params: 4588096
without pretraining.
Epoch 0 [55.1s]: train==[162.26436=55.34947 + 0.00000 + 106.91487]
Epoch 9 [54.4s + 31.8s]: train==[161.29718=54.79246 + 0.00000 + 106.50478], recall=[0.07299, 0.15485], precision=[0.03039, 0.01606], hit=[0.32339, 0.57676], ndcg=[0.11740, 0.20124]
save the weights in path:  weights/last-fm/mf/l0.0001_r1e-05-1e-05
Epoch 19 [51.9s + 31.8s]: train==[160.68282=54.42924 + 0.00000 + 106.25361], recall=[0.07300, 0.15485], precision=[0.03055, 0.01613], hit=[0.32560, 0.57778], ndcg=[0.11828, 0.20217]
save the weights in path:  weights/last-fm/mf/l0.0001_r1e-05-1e-05
Epoch 29 [53.2s + 31.3s]: train==[160.51677=54.48269 + 0.00000 + 106.03420], recall=[0.07317, 0.15477], precision=[0.03065, 0.01614], hit=[0.32577, 0.57647], ndcg=[0.11864, 0.20217]
save the weights in path:  weights/last-fm/mf/l0.0001_r1e-05-1e-05
Epoch 39 [52.9s + 31.7s]: train==[160.40971=54.49051 + 0.00000 + 105.91930], recall=[0.07324, 0.15514], precision=[0.03057, 0.01617], hit=[0.32521, 0.57676], ndcg=[0.11838, 0.20227]
save the weights in path:  weights/last-fm/mf/l0.0001_r1e-05-1e-05
Epoch 49 [53.2s + 31.2s]: train==[160.14568=54.32406 + 0.00000 + 105.82157], recall=[0.07286, 0.15486], precision=[0.03040, 0.01615], hit=[0.32322, 0.57625], ndcg=[0.11756, 0.20161]
Epoch 50 [53.0s]: train==[160.03206=54.22244 + 0.00000 + 105.80960]
Epoch 59 [52.5s + 31.5s]: train==[160.11781=54.36803 + 0.00000 + 105.74966], recall=[0.07274, 0.15539], precision=[0.03050, 0.01619], hit=[0.32386, 0.57647], ndcg=[0.11784, 0.20196]
Epoch 69 [52.2s + 74.8s]: train==[160.12471=54.44283 + 0.00000 + 105.68193], recall=[0.07295, 0.15544], precision=[0.03062, 0.01618], hit=[0.32381, 0.57617], ndcg=[0.11766, 0.20151]
Epoch 79 [52.1s + 31.5s]: train==[159.87572=54.24320 + 0.00000 + 105.63246], recall=[0.07278, 0.15529], precision=[0.03055, 0.01620], hit=[0.32343, 0.57608], ndcg=[0.11759, 0.20176]
Epoch 89 [54.0s + 31.7s]: train==[159.89320=54.29742 + 0.00000 + 105.59575], recall=[0.07306, 0.15581], precision=[0.03061, 0.01624], hit=[0.32369, 0.57710], ndcg=[0.11769, 0.20199]
Epoch 99 [52.2s + 32.0s]: train==[160.03105=54.42572 + 0.00000 + 105.60532], recall=[0.07285, 0.15520], precision=[0.03067, 0.01623], hit=[0.32347, 0.57621], ndcg=[0.11801, 0.20203]
Epoch 100 [52.2s]: train==[159.82585=54.25685 + 0.00000 + 105.56892]
Epoch 109 [52.3s + 31.2s]: train==[159.73843=54.19287 + 0.00000 + 105.54538], recall=[0.07320, 0.15527], precision=[0.03066, 0.01623], hit=[0.32466, 0.57765], ndcg=[0.11806, 0.20216]
Epoch 119 [51.4s + 31.5s]: train==[159.84363=54.30563 + 0.00000 + 105.53796], recall=[0.07308, 0.15544], precision=[0.03055, 0.01623], hit=[0.32322, 0.57901], ndcg=[0.11781, 0.20231]
Epoch 129 [54.4s + 31.7s]: train==[159.67998=54.16376 + 0.00000 + 105.51629], recall=[0.07277, 0.15575], precision=[0.03060, 0.01626], hit=[0.32284, 0.57829], ndcg=[0.11763, 0.20226]
Epoch 139 [51.3s + 31.6s]: train==[159.66524=54.16533 + 0.00000 + 105.49991], recall=[0.07306, 0.15592], precision=[0.03063, 0.01627], hit=[0.32352, 0.57821], ndcg=[0.11803, 0.20261]
Early stopping is trigger at step: 10 log:0.07305653576249746
Best Iter=[3]@[7902.2]	recall=[0.07324	0.10306	0.12443	0.14119	0.15514], precision=[0.03057	0.02384	0.02023	0.01787	0.01617], hit=[0.32521	0.42969	0.49516	0.54197	0.57676], ndcg=[0.11838	0.14951	0.17103	0.18804	0.20227]
Fri Jan  8 21:13:53 PST 2021
Running model_type=bprmf on dataset=yelp2018
Num cores: 20
[n_users, n_items]=[45919, 45538]
[n_train, n_test]=[930032, 253578]
[n_entities, n_relations, n_triples]=[136499, 42, 1853704]
[batch_size, batch_size_kg]=[1024, 2041]
load the pretrained bprmf model parameters.
using pretrained initialization
#params: 5853248
without pretraining.
Epoch 0 [33.0s]: train==[101.61004=35.97368 + 0.00000 + 65.63630]
Epoch 9 [33.5s + 86.2s]: train==[101.13788=35.73746 + 0.00000 + 65.40050], recall=[0.06618, 0.19165], precision=[0.01604, 0.00966], hit=[0.25321, 0.54308], ndcg=[0.08024, 0.15429]
save the weights in path:  weights/yelp2018/mf/l0.0001_r1e-05-1e-05
Epoch 19 [34.4s + 89.0s]: train==[100.87466=35.63294 + 0.00000 + 65.24178], recall=[0.06558, 0.19303], precision=[0.01595, 0.00971], hit=[0.25210, 0.54506], ndcg=[0.08062, 0.15552]
Epoch 29 [33.3s + 58.3s]: train==[100.62318=35.48772 + 0.00000 + 65.13544], recall=[0.06601, 0.19331], precision=[0.01604, 0.00973], hit=[0.25278, 0.54523], ndcg=[0.08167, 0.15649]
Epoch 39 [34.4s + 59.4s]: train==[100.52065=35.45945 + 0.00000 + 65.06113], recall=[0.06636, 0.19271], precision=[0.01609, 0.00972], hit=[0.25358, 0.54423], ndcg=[0.08097, 0.15549]
save the weights in path:  weights/yelp2018/mf/l0.0001_r1e-05-1e-05
Epoch 49 [32.9s + 58.2s]: train==[100.40384=35.40328 + 0.00000 + 65.00056], recall=[0.06571, 0.19306], precision=[0.01597, 0.00973], hit=[0.25169, 0.54469], ndcg=[0.08104, 0.15604]
Epoch 50 [33.8s]: train==[100.50524=35.50961 + 0.00000 + 64.99560]
Epoch 59 [32.5s + 57.1s]: train==[100.61645=35.65675 + 0.00000 + 64.95977], recall=[0.06668, 0.19292], precision=[0.01615, 0.00972], hit=[0.25487, 0.54454], ndcg=[0.08186, 0.15619]
save the weights in path:  weights/yelp2018/mf/l0.0001_r1e-05-1e-05
Epoch 69 [33.7s + 90.3s]: train==[100.33400=35.39727 + 0.00000 + 64.93679], recall=[0.06611, 0.19356], precision=[0.01606, 0.00976], hit=[0.25306, 0.54591], ndcg=[0.08142, 0.15641]
Epoch 79 [33.4s + 57.1s]: train==[100.16245=35.25953 + 0.00000 + 64.90287], recall=[0.06580, 0.19375], precision=[0.01601, 0.00976], hit=[0.25223, 0.54719], ndcg=[0.08093, 0.15621]
Epoch 89 [32.5s + 58.0s]: train==[100.22214=35.34497 + 0.00000 + 64.87713], recall=[0.06598, 0.19366], precision=[0.01606, 0.00978], hit=[0.25223, 0.54693], ndcg=[0.08163, 0.15683]
Epoch 99 [32.2s + 58.1s]: train==[100.28051=35.42575 + 0.00000 + 64.85481], recall=[0.06626, 0.19403], precision=[0.01608, 0.00978], hit=[0.25408, 0.54789], ndcg=[0.08167, 0.15682]
Epoch 100 [32.7s]: train==[100.40252=35.55068 + 0.00000 + 64.85184]
Epoch 109 [32.2s + 58.2s]: train==[100.38088=35.53524 + 0.00000 + 64.84568], recall=[0.06608, 0.19381], precision=[0.01610, 0.00978], hit=[0.25313, 0.54706], ndcg=[0.08108, 0.15619]
Epoch 119 [32.2s + 57.0s]: train==[100.23949=35.39890 + 0.00000 + 64.84064], recall=[0.06594, 0.19343], precision=[0.01608, 0.00977], hit=[0.25367, 0.54630], ndcg=[0.08135, 0.15640]
Epoch 129 [31.9s + 57.5s]: train==[100.09642=35.27218 + 0.00000 + 64.82421], recall=[0.06640, 0.19327], precision=[0.01615, 0.00975], hit=[0.25493, 0.54486], ndcg=[0.08231, 0.15682]
Epoch 139 [32.5s + 57.7s]: train==[100.35462=35.53571 + 0.00000 + 64.81890], recall=[0.06628, 0.19360], precision=[0.01611, 0.00978], hit=[0.25413, 0.54641], ndcg=[0.08204, 0.15699]
Epoch 149 [32.0s + 59.5s]: train==[100.21278=35.41229 + 0.00000 + 64.80051], recall=[0.06615, 0.19307], precision=[0.01607, 0.00975], hit=[0.25332, 0.54573], ndcg=[0.08167, 0.15653]
Epoch 150 [32.6s]: train==[100.14159=35.34768 + 0.00000 + 64.79394]
Epoch 159 [33.6s + 58.1s]: train==[100.26138=35.46528 + 0.00000 + 64.79613], recall=[0.06628, 0.19296], precision=[0.01611, 0.00974], hit=[0.25345, 0.54669], ndcg=[0.08220, 0.15702]
Early stopping is trigger at step: 10 log:0.06627800805054107
Best Iter=[5]@[6279.7]	recall=[0.06668	0.10823	0.14068	0.16854	0.19292], precision=[0.01615	0.01329	0.01168	0.01055	0.00972], hit=[0.25487	0.36977	0.44429	0.49978	0.54454], ndcg=[0.08186	0.10910	0.12826	0.14337	0.15619]
Finished